{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1257258b-c279-43a3-b73f-1efd12c69382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers.utils import is_flash_attn_2_available \n",
    "from nn_rag import Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c939bcdb-2dd8-4e46-98c1-b14dd45bb2e7",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e6a503a-42b4-449e-916d-0457b20a634d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nn_rag.components.retrieval.Retrieval at 0x7fe108225420>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag = Retrieval.from_memory()\n",
    "rag.set_source_uri('chroma:///hadron/data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd8c44a-b171-4480-8780-7dd3ca9098de",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c84f33be-92be-48a3-93e9-5e5ddb53e96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e41f53b64f41018ea7fa083117e6e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_id = 'google/gemma-2b-it'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    attn_implementation = \"sdpa\"\n",
    "    \n",
    "# 4 bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16, # gain back some speed \n",
    "    bnb_4bit_quant_type=\"nf4\", # weights initialized with normal distribution\n",
    "    bnb_4bit_use_double_quant=True, # better memory footprint\n",
    "    llm_int8_enable_fp32_cpu_offload=True, # offload weights cross GPU and CPU\n",
    ")\n",
    "\n",
    "# instantiate the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=None,\n",
    "    device_map=device,\n",
    "    attn_implementation=attn_implementation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "492f965e-0891-41f1-b542-ba9e2fe5a3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Model Size: 2,506,172,416 parameters'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_num_params(model: torch.nn.Module):\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "\n",
    "f\"Model Size: {get_model_num_params(model):,.0f} parameters\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409a3a22-21fb-442d-83dc-b5ae2eb585af",
   "metadata": {},
   "source": [
    "### Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4fa98ac-f8d0-462e-9b4e-69f181791e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter(query: str, context_items: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Augments query with text-based context from context_items.\n",
    "    \"\"\"\n",
    "    # Join context items into one dotted paragraph\n",
    "    context = \"- \" + \"\\n- \".join([item for item in context_items])\n",
    "\n",
    "    # Create a base prompt with examples to help the model\n",
    "    base_prompt = \"\"\"I am a professional summarizer with an academic audience. \n",
    "Based on the following context items, please answer the query. \n",
    "Give yourself room to think by extracting relevant passages from the context before answering the query. \n",
    "Don't return the thinking, only return the answer. Make sure your answers are as explanatory as possible.\n",
    "Now use the following context items to answer the user query:\n",
    "{context}\n",
    "Relevant passages: <extract relevant passages from the context here>\n",
    "User query: {query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Update base prompt with context items and query   \n",
    "    base_prompt = base_prompt.format(context=context, query=query)\n",
    "\n",
    "    # Create prompt template for instruction-tuned model\n",
    "    dialogue_template = [\n",
    "        {\"role\": \"user\", \"content\": base_prompt}\n",
    "    ]\n",
    "\n",
    "    # Apply the chat template\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                          tokenize=False,\n",
    "                                          add_generation_prompt=True)\n",
    "\n",
    "    return prompt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce0300e-ae5b-44d2-bb83-897875e37402",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65b57981-6397-4b86-9176-e19941b75a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query, \n",
    "        temperature=0.7,\n",
    "        max_new_tokens=512,\n",
    "        format_answer_text=True, \n",
    "        return_answer_only=True):\n",
    "    \n",
    "    # Get the query answers from the embedding\n",
    "    context_items = rag.tools.query_reranker(query=query, bi_limit=20, cross_limit=10)['text'].to_pylist()\n",
    "    \n",
    "    # Format the prompt with context items\n",
    "    prompt = prompt_formatter(query=query, context_items=context_items)\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Set tokenize terminators\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    # Generate an output of tokens\n",
    "    outputs = model.generate(**input_ids,\n",
    "                             eos_token_id=terminators,\n",
    "                             temperature=temperature,\n",
    "                             do_sample=True,\n",
    "                             max_new_tokens=max_new_tokens,\n",
    "                             top_p=0.9,\n",
    "                            )\n",
    "    \n",
    "    # Turn the output tokens into text\n",
    "    output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "    if format_answer_text:\n",
    "        # Replace special tokens and unnecessary help message\n",
    "        output_text = output_text.replace(prompt, \"\").replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"Sure, here is the answer to the user query:\\n\\n\", \"\")\n",
    "\n",
    "    # Only return the answer without the context items\n",
    "    if return_answer_only:\n",
    "        return output_text\n",
    "    \n",
    "    return output_text, context_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bab64716-d95a-494d-b2bb-aa204fe3f7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = outputs[0][input_ids.shape[-1]:]\n",
    "# print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9418952-94b3-464e-822a-2bcbbc184841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9addb924-2d25-4df5-abd3-a6caef683803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "questions = [\n",
    "    \"What are the steps of the responsible fine-tuning flow?\",\n",
    "    \"What is the impact and best practices for mitigating ethical and responsibility risks in the deployment of artificial intelligence (AI) models?\",\n",
    "    \"How does the broader impact of AI technologies effect potential biases, privacy concerns, and the societal implications of AI deployment?\",\n",
    "    \"How should I design AI systems that are fair and do not perpetuate existing biases?\",\n",
    "    \"What is the importance of transparency through clear communication in AI systems?\",\n",
    "    \"How does privacy and security secure against malicious attacks in the deployment of artificial intelligence (AI) models?\",\n",
    "    \"How does the guide addresse the significance of accountability in responsible AI\",    \n",
    "]\n",
    "\n",
    "query = random.choice(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0bb08f2-6bf5-4bfe-910b-d5fe1b9b9047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the steps of the responsible fine-tuning flow?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/neural/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "Sure, here's a summary of the steps of the responsible fine-tuning flow:  **Step\n",
      "1: Define Content Policies & Mitigations**  * Define content policies and\n",
      "mitigations based on the intended use and audience. * Evaluate these policies in\n",
      "light of the product domain and specific sectors or regions.  **Step 2: Prepare\n",
      "Data**  * Prepare and preprocess a clean dataset that is representative of the\n",
      "target domain. * Tokenize text, handle special characters, remove unnecessary\n",
      "information, and split the dataset into training, validation, and testing sets.\n",
      "* Ensure data is representative of the target user base.  **Step 3: Train the\n",
      "Model**  * Train the model on the prepared data. * Evaluate and improve\n",
      "performance.  **Step 4: Evaluate and Improve Performance**  * Define content\n",
      "policies and mitigations to ensure the model's outputs align with the intended\n",
      "purpose. * Implement Reinforcement Learning from Human Feedback (RLHF)\n",
      "mechanisms to align the model's output with user expectations and values.\n",
      "\n",
      "Context items:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['There are many complementary types of evaluations that are useful for measuring risks in models, including automatic benchmarks, manual annotations by human raters, and evaluations using an LLM itself as a rater. The Holistic Evaluation of Language Models discusses some of the commonly used automatic benchmarks. Evaluation strategies and processes to improve performance can include: • Automatic evaluation leverages automatic benchmarks and classifiers to judge the output with respect to a specific category of risk. Manual evaluation leverages human annotators or subject matter experts to judge the model’s output.',\n",
       " 'Sentiment analysis: A model can be fine-tuned on a dataset of labeled text reviews (positive or negative sentiment) to recognize sentiment and perform analysis to understand user satisfaction. By training the model on this task- specific dataset, it can learn to predict sentiment in text accurately. These examples showcase how fine-tuning an LLM can be used to specialize the model’s capabilities for specific use cases, improving its performance and making it more suitable for specific applications. The choice of the foundation model and the task- specific dataset plays a crucial role in achieving the desired results.',\n",
       " 'The THE RESPONSIBLE FINE-TUNING FLOW STEP 2: PREPARE DATA Developing downstream applications of LLMs begins with taking steps to consider the potential limitations, privacy implications, and representativeness of data for a specific use case. Begin by preparing and preprocessing a clean dataset that is representative of the target domain. This involves tokenizing the text, handling special characters, removing unnecessary information, and splitting the dataset into training, validation, and testing sets. This step may also involve ensuring that data are representative of the end users in the deployment context, for instance, by ensuring there are enough examples from relevant languages if you plan to deploy your product in a non-English speaking market.',\n",
       " '2 8 JULY 2023 The responsible fine-tuning flow Here are the general steps needed to responsibly fine-tune an LLM for alignment, guided at a high level by Meta’s Responsible AI framework: 1. Define content policies & mitigations 2. Prepare data 3. Train the model 4. Evaluate and improve performance STEP 1: DEFINE CONTENT POLICIES & MITIGATIONS Based on the intended use and audience for your product, a content policy will define what content is allowable and may outline safety limitations on producing illegal, violent, or harmful content. These limits should be evaluated in light of the product domain, as specific sectors and regions may have different laws or standards.',\n",
       " 'Instead of removing data, focusing on the representativeness of the data can help prevent a fine-tuned model from perpetuating biases in its generated outputs; what is considered representative 10 JULY 2023 Reinforcement Learning from Human Feedback (RLHF) To align the output of LLMs with user expectations and values, one approach that developers should consider is implementing Reinforcement Learning from Human Feedback (RLHF) mechanisms.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Answer query with context and return context \n",
    "answer, context_items = ask(query=query, \n",
    "                            temperature=0.8,\n",
    "                            max_new_tokens=512,\n",
    "                            return_answer_only=False)\n",
    "\n",
    "print(f\"Answer:\")\n",
    "print_wrapped(answer)\n",
    "print(f\"\\nContext items:\")\n",
    "context_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6516f0f9-6358-4803-acc5-47333a1597cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
