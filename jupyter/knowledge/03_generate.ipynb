{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1257258b-c279-43a3-b73f-1efd12c69382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers.utils import is_flash_attn_2_available \n",
    "from nn_rag import Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c939bcdb-2dd8-4e46-98c1-b14dd45bb2e7",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e6a503a-42b4-449e-916d-0457b20a634d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nn_rag.components.retrieval.Retrieval at 0x7f864bb68e20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag = Retrieval.from_memory()\n",
    "rag.set_source_uri('chroma:///hadron/data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd8c44a-b171-4480-8780-7dd3ca9098de",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c84f33be-92be-48a3-93e9-5e5ddb53e96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deea373f0c204ffb9004c229433921f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_id = 'google/gemma-2b-it'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    attn_implementation = \"sdpa\"\n",
    "    \n",
    "# 4 bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16, # gain back some speed \n",
    "    bnb_4bit_quant_type=\"nf4\", # weights initialized with normal distribution\n",
    "    bnb_4bit_use_double_quant=True, # better memory footprint\n",
    "    llm_int8_enable_fp32_cpu_offload=True, # offload weights cross GPU and CPU\n",
    ")\n",
    "\n",
    "# instantiate the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=None,\n",
    "    device_map=device,\n",
    "    attn_implementation=attn_implementation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "492f965e-0891-41f1-b542-ba9e2fe5a3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Model Size: 2,506,172,416 parameters'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_num_params(model: torch.nn.Module):\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "\n",
    "f\"Model Size: {get_model_num_params(model):,.0f} parameters\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409a3a22-21fb-442d-83dc-b5ae2eb585af",
   "metadata": {},
   "source": [
    "### Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4fa98ac-f8d0-462e-9b4e-69f181791e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter(query: str, context_items: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Augments query with text-based context from context_items.\n",
    "    \"\"\"\n",
    "    # Join context items into one dotted paragraph\n",
    "    context = \"- \" + \"\\n- \".join([item for item in context_items])\n",
    "\n",
    "    # Create a base prompt with examples to help the model\n",
    "    base_prompt = \"\"\"I am a professional summarizer with an academic audience. \n",
    "Based on the following context items, please answer the query. \n",
    "Give yourself room to think by extracting relevant passages from the context before answering the query. \n",
    "Don't return the thinking, only return the answer. Make sure your answers are as explanatory as possible.\n",
    "Now use the following context items to answer the user query:\n",
    "{context}\n",
    "Relevant passages: <extract relevant passages from the context here>\n",
    "User query: {query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Update base prompt with context items and query   \n",
    "    base_prompt = base_prompt.format(context=context, query=query)\n",
    "\n",
    "    # Create prompt template for instruction-tuned model\n",
    "    dialogue_template = [\n",
    "        {\"role\": \"user\", \"content\": base_prompt}\n",
    "    ]\n",
    "\n",
    "    # Apply the chat template\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                          tokenize=False,\n",
    "                                          add_generation_prompt=True)\n",
    "\n",
    "    return prompt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce0300e-ae5b-44d2-bb83-897875e37402",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65b57981-6397-4b86-9176-e19941b75a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query, \n",
    "        temperature=0.7,\n",
    "        max_new_tokens=512,\n",
    "        format_answer_text=True, \n",
    "        return_answer_only=True):\n",
    "    \n",
    "    # Get the query answers from the embedding\n",
    "    context_items = rag.tools.query_reranker(query=query, bi_limit=20, cross_limit=10)['text'].to_pylist()\n",
    "    \n",
    "    # Format the prompt with context items\n",
    "    prompt = prompt_formatter(query=query, context_items=context_items)\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Set tokenize terminators\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    # Generate an output of tokens\n",
    "    outputs = model.generate(**input_ids,\n",
    "                             eos_token_id=terminators,\n",
    "                             temperature=temperature,\n",
    "                             do_sample=True,\n",
    "                             max_new_tokens=max_new_tokens,\n",
    "                             top_p=0.9,\n",
    "                            )\n",
    "    \n",
    "    # Turn the output tokens into text\n",
    "    output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "    if format_answer_text:\n",
    "        # Replace special tokens and unnecessary help message\n",
    "        output_text = output_text.replace(prompt, \"\").replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"Sure, here is the answer to the user query:\\n\\n\", \"\")\n",
    "\n",
    "    # Only return the answer without the context items\n",
    "    if return_answer_only:\n",
    "        return output_text\n",
    "    \n",
    "    return output_text, context_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bab64716-d95a-494d-b2bb-aa204fe3f7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = outputs[0][input_ids.shape[-1]:]\n",
    "# print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9418952-94b3-464e-822a-2bcbbc184841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9addb924-2d25-4df5-abd3-a6caef683803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "questions = [\n",
    "    \"What are the steps of the responsible fine-tuning flow?\",\n",
    "    \"What is the impact and best practices for mitigating ethical and responsibility risks in the deployment of artificial intelligence (AI) models?\",\n",
    "    \"How does the broader impact of AI technologies effect potential biases, privacy concerns, and the societal implications of AI deployment?\",\n",
    "    \"How should I design AI systems that are fair and do not perpetuate existing biases?\",\n",
    "    \"What is the importance of transparency through clear communication in AI systems?\",\n",
    "    \"How does privacy and security secure against malicious attacks in the deployment of artificial intelligence (AI) models?\",\n",
    "    \"How does the guide addresse the significance of accountability in responsible AI\",    \n",
    "]\n",
    "\n",
    "query = random.choice(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0bb08f2-6bf5-4bfe-910b-d5fe1b9b9047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How should I design AI systems that are fair and do not perpetuate existing biases?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/neural/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "The context does not provide any information about how to design AI systems that\n",
      "are fair and do not perpetuate existing biases, so I cannot answer this question\n",
      "from the provided context.\n",
      "\n",
      "Context items:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Fine-tuning adapts the model to domain- or application- specific requirements and introduces additional layers of safety mitigations. Examples of fine-tuning for a pretrained LLM include: • Text summarization: By using a pretrained language model, the model can be fine-tuned on a dataset that includes pairs of long-form documents and corresponding summaries. This fine-tuned model can then generate concise summaries for new documents. Question answering: Fine-tuning a language model on a Q&A dataset such as SQuAD (Stanford Question Answering Dataset) allows the model to learn how to answer questions based on a given context paragraph. The fine-tuned model can then be used to answer questions on various topics.',\n",
       " '13 JULY 2023 Address input- and output-level risks Without proper safeguards at the input and output levels, it is hard to ensure that the model will respond properly to adversarial inputs and will be protected from efforts to circumvent content policies and safeguard measures (“jailbreaking”). Mitigations at the output level can also act as a safeguard against generating high-risk or policy-violating content. Enforcement of content policies can be managed through automated systems and manual analysis of samples and reports. Automated systems may include machine learning and rule-based classifiers for filtering prompt inputs or system outputs. Usage or consequence policies may be defined for when users repeatedly violate those policies.',\n",
       " 'Representativeness of data is dependent on the use case and should be assessed accordingly. When fine-tuning for a specific use case it can be beneficial to examine training data for biases, such as gender, racial, linguistic, cultural or other biases. Understanding these patterns is important but it may not always be optimal to filter out all problematic content in training data due to the unintended consequences this filtering may have on subsequent performance and safety mitigations, such as prompt engineering.',\n",
       " 'If you’re a developer who is not certain of a particular use case for which you would want to use the model, consider focusing on use cases that improve the lives of people and society, taking into consideration different ethical principles and values. Developing or adopting an internal risk assessment process can help identify potential risks for a specific use case and should focus on how your product’s end users and others could be affected. This understanding is critical for evaluating in-context safety for your product deployment, and can take forms such as surveys and interviews of potential users or market analysis of similar product applications.',\n",
       " 'To mitigate these risks, carefully design the fine-tuning process by curating a high-quality dataset that is representative of your use case, conduct rigorous evaluations, and test your fine-tuned model’s potential use via red teaming (covered in step four - Evaluate and improve performance). STEP 3: TRAIN THE MODEL Fine-tuning involves training the model for a limited number of iterations. Once a pretrained model is loaded in the environment for fine-tuning, the training process involves setting up hyperparameters like epochs, batch size, and learning rate. The data are passed through the model, loss is computed, and weights are updated through backpropagation.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Answer query with context and return context \n",
    "answer, context_items = ask(query=query, \n",
    "                            temperature=0.7,\n",
    "                            max_new_tokens=768,\n",
    "                            return_answer_only=False)\n",
    "\n",
    "print(f\"Answer:\")\n",
    "print_wrapped(answer)\n",
    "print(f\"\\nContext items:\")\n",
    "context_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6516f0f9-6358-4803-acc5-47333a1597cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
