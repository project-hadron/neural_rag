{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "962e2eab-fef2-4393-8e03-836e7921a99d",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# saves you having to use print as all exposed variables are printed in the cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "116bbfb0-cf0d-4946-a9ab-0430479d0e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warning message\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68bdaf3a-78fb-438b-9590-d4feb46ad151",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "from nn_rag import Knowledge, Controller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d62f37-c1fe-4d91-aadc-231253ccf97f",
   "metadata": {},
   "source": [
    "### Milvus Vector Params\n",
    "\n",
    "    URI example\n",
    "        uri = \"milvus://host:port/database?collection=<name>&doc_ref=<name>\"\n",
    "\n",
    "    params:\n",
    "        collection: The name of the collection\n",
    "        doc_ref: a name to reference the document\n",
    "\n",
    "    Environment Hyperparams:\n",
    "        MILVUS_EMBEDDING_NAME\n",
    "        MILVUS_EMBEDDING_DEVICE\n",
    "        MILVUS_EMBEDDING_BATCH_SIZE\n",
    "        MILVUS_EMBEDDING_DIM\n",
    "        MILVUS_INDEX_CLUSTERS\n",
    "        MILVUS_INDEX_SIMILARITY_TYPE\n",
    "        MILVUS_QUERY_SEARCH_LIMIT\n",
    "        MILVUS_QUERY_NUM_SIMILARITY\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46761902-7106-4628-9b12-0ff133fe2041",
   "metadata": {},
   "source": [
    "### Set parameters as environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52a63c1a-db81-4b74-83dc-0e84b9e5bb48",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "# os.environ['HADRON_KNOWLEDGE_SOURCE_URI'] = './hadron/source/Gen AI Best Practices.pdf'\n",
    "os.environ['HADRON_KNOWLEDGE_SOURCE_URI'] = \"./hadron/source/Human-Nutrition-2020-Edition.pdf\"\n",
    "os.environ['HADRON_KNOWLEDGE_EMBED_URI'] = 'milvus://localhost:19530/rai?collection=demo&doc_ref=human_nutrition'\n",
    "\n",
    "# Parameters\n",
    "os.environ['HADRON_KNOWLEDGE_EMBEDDING_NAME'] = 'all-mpnet-base-v2'\n",
    "os.environ['HADRON_KNOWLEDGE_CHUNK_SIZE'] = '300'\n",
    "\n",
    "# Vector Db\n",
    "os.environ['MILVUS_EMBEDDING_NAME'] = 'all-mpnet-base-v2'\n",
    "os.environ['MILVUS_EMBEDDING_DIM'] = '768'\n",
    "os.environ['MILVUS_EMBEDDING_DEVICE'] = 'cpu'\n",
    "os.environ['MILVUS_QUERY_SEARCH_LIMIT'] = '5'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb5c8ff-3497-4213-a0fc-db712cb4a8a8",
   "metadata": {},
   "source": [
    "### Instantiate capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b85ae8c1-598a-4764-97b0-7b882e3fadc8",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "kn = Knowledge.from_env('demo', has_contract=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89a26a14-a209-4c55-9925-125667c1f168",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "kn.set_description('a reusable component to embed documentation to build an AI knowledge base for a RAG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "361cb284-9dbd-461b-9fbe-6abf87fed6c1",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nn_rag.components.knowledge.Knowledge at 0x7fdab41cc2b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<nn_rag.components.knowledge.Knowledge at 0x7fdab41cc2b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kn.set_source_uri(\"${HADRON_KNOWLEDGE_SOURCE_URI}\")\n",
    "kn.set_persist_uri('${HADRON_KNOWLEDGE_EMBED_URI}')\n",
    "kn.add_connector_uri('query', '${HADRON_KNOWLEDGE_EMBED_URI}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e1ed99-72a1-449c-a8cb-1121e86f61b5",
   "metadata": {},
   "source": [
    "### Delete the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35240781-ed05-461d-8d2a-cfb7dddf7fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "kn.remove_collection(kn.CONNECTOR_PERSIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fe3fce-bcba-4c3c-a422-09595a350230",
   "metadata": {},
   "source": [
    "### Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "531af383-5d1c-4a86-adaf-c049d1cc7917",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "doc = kn.load_source_canonical()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29e41f1b-9f7d-4679-866b-345d7ea9fdbb",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# tidy the text\n",
    "doc = kn.tools.pattern_replace(doc, 'text', '\\n', ' ', intent_order=-1)\n",
    "doc = kn.tools.pattern_replace(doc, 'text', '  ', ' ', intent_order=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87bb432-0fd3-4ac4-b4ea-8d766905a62f",
   "metadata": {},
   "source": [
    "### Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c36146e-e73c-4596-88f2-9936a8bc984d",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 1357747 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[43mkn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_profiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m$\u001b[39;49m\u001b[38;5;132;43;01m{HADRON_KNOWLEDGE_EMBEDDING_NAME}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintent_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/neural/lib/python3.10/site-packages/nn_rag/intent/knowledge_intent.py:200\u001b[0m, in \u001b[0;36mKnowledgeIntent.text_profiler\u001b[0;34m(self, canonical, header, embedding_name, save_intent, intent_level, intent_order, replace_intent, remove_duplicates)\u001b[0m\n\u001b[1;32m    198\u001b[0m sentences \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m text:\n\u001b[0;32m--> 200\u001b[0m     sents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msents)\n\u001b[1;32m    201\u001b[0m     sents \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sents]\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m num, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sents):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/neural/lib/python3.10/site-packages/spacy/language.py:1037\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1018\u001b[0m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1022\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;124;03m    is preserved.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1037\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1039\u001b[0m         component_cfg \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/anaconda3/envs/neural/lib/python3.10/site-packages/spacy/language.py:1128\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc_like\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 1128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_like\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Doc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\u001b[38;5;241m.\u001b[39mfrom_bytes(doc_like)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/neural/lib/python3.10/site-packages/spacy/language.py:1117\u001b[0m, in \u001b[0;36mLanguage.make_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Turn a text into a Doc object.\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m \n\u001b[1;32m   1113\u001b[0m \u001b[38;5;124;03mtext (str): The text to process.\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;124;03mRETURNS (Doc): The processed doc.\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[0;32m-> 1117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1118\u001b[0m         Errors\u001b[38;5;241m.\u001b[39mE088\u001b[38;5;241m.\u001b[39mformat(length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(text), max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[1;32m   1119\u001b[0m     )\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text)\n",
      "\u001b[0;31mValueError\u001b[0m: [E088] Text of length 1357747 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "sentences = kn.tools.text_profiler(doc, embedding_name='${HADRON_KNOWLEDGE_EMBEDDING_NAME}' , intent_order=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba07b61d-1563-438d-98ff-0571de822c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f52b0c-a6e0-447d-b567-fdf6c04e513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.min(sentences['char_count']).as_py()\n",
    "pc.mean(sentences['char_count']).as_py()\n",
    "pc.max(sentences['char_count']).as_py()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b39e278-e7ae-474a-812f-7682c52f5bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.count(pc.filter(sentences['sentence_score'], pc.greater(sentences['sentence_score'], 0.90))).as_py()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d00b208-3e72-435c-bdf1-deaa89c8de35",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78fdc2-df14-4fad-bcf2-60490b6e9629",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "chunks = kn.tools.sentence_chunks(sentences, char_chunk_size='${HADRON_KNOWLEDGE_CHUNK_SIZE}', temperature=0.9, intent_order=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758605d5-3835-47e7-b50c-36d40d22fe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e867ef17-4c04-4e58-9c7d-07121ea14ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.min(chunks['chunk_char_count']).as_py()\n",
    "pc.mean(chunks['chunk_char_count']).as_py()\n",
    "pc.max(chunks['chunk_char_count']).as_py()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caee61a-6506-4025-99d5-d2969a320a10",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caf4977-4398-45f5-9a48-af84a21cf7a7",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# kn.save_persist_canonical(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17997d56-64f5-4124-8a3d-0d3cc13c0d6b",
   "metadata": {},
   "source": [
    "### Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc796f5-7f22-478a-857d-60f50565b590",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "ctr = Controller.from_env(has_contract=False)\n",
    "ctr.set_use_case(title='Rag Demo', domain='General', \n",
    "                 overview='A pipeline that allows the collection of documentation to embed for a RAG catalog.', \n",
    "                 situation='HUB requirement for better access to documentation', \n",
    "                 opportunity='Improve accessibility through a RAG', \n",
    "                 actions='build a catalog of embedded documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c448c458-551f-4471-a162-4bec81298110",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "ctr.register.knowledge(task_name='demo', intent_level='knowledge_demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebb0eb6-8b8c-432a-a33a-109eeefed64c",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# kn.remove_canonical(kn.CONNECTOR_PERSIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de680ad9-b26f-4821-a146-5f60dc60218a",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "ctr.run_controller()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e792c-5d0a-4d9c-bad0-59e409c18097",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce53daf-dc7c-4632-acd9-44565f1ca542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7da657-4642-4323-98e3-aa34cbce30bd",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "query = \"enabling successful responsible AI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b25025-7706-4622-b42d-ccea534b020f",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "answer = kn.load_canonical('query', query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aa4e9d-333d-48e1-bb3a-c0e433d0d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer.column_names\n",
    "answer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6260368-4485-4ec7-9630-61fed8540436",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Results:\")\n",
    "for idx in range():\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "    print(\"Text:\")\n",
    "    print_wrapped(pages_and_chunks[idx][\"sentence_chunk\"])\n",
    "    # Print the page number too so we can reference the textbook further (and check the results)\n",
    "    print(f\"Page number: {pages_and_chunks[idx]['page_number']}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
