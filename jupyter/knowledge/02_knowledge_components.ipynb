{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962e2eab-fef2-4393-8e03-836e7921a99d",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# saves you having to use print as all exposed variables are printed in the cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "116bbfb0-cf0d-4946-a9ab-0430479d0e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warning message\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68bdaf3a-78fb-438b-9590-d4feb46ad151",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "from nn_rag import Knowledge, Re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d62f37-c1fe-4d91-aadc-231253ccf97f",
   "metadata": {},
   "source": [
    "### Chroma Vector Params\n",
    "\n",
    "        URI example in-memory\n",
    "            uri = \"chromadb:///<collection>?reference=<name>\"\n",
    "\n",
    "        params:\n",
    "            collection: The name of the collection\n",
    "            reference: a prefix name to reference the document vector\n",
    "\n",
    "        Environment:\n",
    "            CHROMA_EMBEDDING_QUANTIZE\n",
    "            CHROMA_QUERY_SEARCH_LIMIT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb5c8ff-3497-4213-a0fc-db712cb4a8a8",
   "metadata": {},
   "source": [
    "### Instantiate capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b85ae8c1-598a-4764-97b0-7b882e3fadc8",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "kn = Knowledge.from_env('demo', has_contract=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "361cb284-9dbd-461b-9fbe-6abf87fed6c1",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "tbl = kn.set_source_uri(\"./hadron/source/llama-Responsible-Use-Guide.pdf\").load_source_canonical()\n",
    "kn.set_persist_uri('chroma:///')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7a6dec-cfc8-4d94-9b75-9bb937348452",
   "metadata": {},
   "source": [
    "### Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30a772c2-695d-4ad5-9c88-35b3b130d4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = kn.tools.replace_on_pattern(tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d00b208-3e72-435c-bdf1-deaa89c8de35",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be78fdc2-df14-4fad-bcf2-60490b6e9629",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "chunks = kn.tools.text_to_chunks(doc, char_chunk_size='300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caee61a-6506-4025-99d5-d2969a320a10",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1caf4977-4398-45f5-9a48-af84a21cf7a7",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "kn.save_persist_canonical(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e792c-5d0a-4d9c-bad0-59e409c18097",
   "metadata": {},
   "source": [
    "----------------\n",
    "## Chroma Vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302065d3-638c-4500-893c-b104c9a2b2d3",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fce53daf-dc7c-4632-acd9-44565f1ca542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    return wrapped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5299de0-8f42-4084-a359-fa3d377206d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "questions = [\n",
    "    \"1. What are the core principles of responsible AI mentioned in the guide?\",\n",
    "    \"2. How does Meta's open approach contribute to AI innovation?\",\n",
    "    \"3. What are the stages of responsible LLM product development according to the guide?\",\n",
    "    \"4. What are some examples of product-specific fine-tuning for LLMs?\",\n",
    "    \"5. What considerations should be taken into account when defining content policies for LLMs?\",\n",
    "    \"6. What are the benefits of democratizing access to large language models, as stated in the guide?\"\n",
    "]\n",
    "\n",
    "query = random.choice(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eede9513-fb35-4708-b74d-10ce21e9113a",
   "metadata": {},
   "source": [
    "### Model Answers\n",
    "1. **Core principles of responsible AI:**\n",
    "   The guide outlines core principles of responsible AI, which include fairness and inclusion, robustness and safety, privacy and security, and transparency and control. Additionally, it emphasizes the importance of governance and accountability mechanisms to ensure these principles are upheld throughout the development and deployment of AI systems.\n",
    "\n",
    "2. **Meta's open approach and AI innovation:**\n",
    "   Meta's open approach to AI innovation involves open-sourcing code and datasets, contributing to the AI community's infrastructure, and making large language models available for research. This approach fosters a vibrant AI-innovation ecosystem, driving breakthroughs in various sectors and enabling exploratory research and large-scale production deployment. It also draws upon the collective wisdom and diversity of the AI community to improve and democratize AI technology.\n",
    "\n",
    "3. **Stages of responsible LLM product development:**\n",
    "   The guide identifies four stages of responsible LLM product development: determining the use case, fine-tuning for the product, addressing input- and output-level risks, and building transparency and reporting mechanisms in user interactions. Each stage involves specific considerations and mitigation strategies to ensure the safe and effective deployment of LLM-powered products.\n",
    "\n",
    "4. **Examples of product-specific fine-tuning:**\n",
    "   Product-specific fine-tuning examples provided in the guide include text summarization, question answering, and sentiment analysis. For instance, a pretrained language model can be fine-tuned on a dataset of long-form documents and summaries for text summarization, on a Q&A dataset for answering questions, and on labeled text reviews for sentiment analysis. These examples demonstrate how fine-tuning can tailor a model's capabilities to specific use cases, enhancing performance and applicability.\n",
    "\n",
    "5. **Considerations for defining content policies:**\n",
    "   When defining content policies for LLMs, developers should consider the intended use and audience of their product, legal and safety limitations, and the needs of specific user communities. Content policies should outline allowable content and safety limitations, which will guide data annotation and safety fine-tuning. It is also important to address potential biases in human feedback and data annotation processes to ensure fairness and objectivity.\n",
    "\n",
    "6. **Benefits of democratizing access to large language models:**\n",
    "   Democratizing access to large language models, as discussed in the guide, reduces barriers to entry for small businesses and fosters innovation across various sectors. By making these models widely available, small organizations can leverage advanced AI technology without incurring prohibitive costs, leading to economic growth and a more level playing field. This approach also promotes collaboration and collective improvement of AI models, ensuring that advancements benefit a broader range of users and applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d1aaaab-ab57-47a2-884a-b382347bb41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 5. What considerations should be taken into account when defining content policies for LLMs?\n",
      "\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "The connector name query can't be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mkn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_canonical\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(answer\u001b[38;5;241m.\u001b[39mnum_rows):\n\u001b[1;32m      6\u001b[0m     s \u001b[38;5;241m=\u001b[39m answer\u001b[38;5;241m.\u001b[39mslice(i,\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/neural/lib/python3.10/site-packages/nn_rag/components/abstract_common_component.py:102\u001b[0m, in \u001b[0;36mAbstractCommonComponent.load_canonical\u001b[0;34m(self, connector_name, reset_changed, has_changed, return_empty, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_canonical\u001b[39m(\u001b[38;5;28mself\u001b[39m, connector_name: \u001b[38;5;28mstr\u001b[39m, reset_changed: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, has_changed: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     93\u001b[0m                    return_empty: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pa\u001b[38;5;241m.\u001b[39mTable:\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"returns the canonical of the referenced connector\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    :param connector_name: the name or label to identify and reference the connector\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    :param kwargs: arguments to be passed to the handler on load\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     canonical \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_canonical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnector_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnector_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_changed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_changed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mhas_changed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_changed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_empty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m canonical\n",
      "File \u001b[0;32m/opt/anaconda3/envs/neural/lib/python3.10/site-packages/ds_core/components/abstract_component.py:862\u001b[0m, in \u001b[0;36mAbstractComponent.load_canonical\u001b[0;34m(self, connector_name, reset_changed, has_changed, return_empty, **kwargs)\u001b[0m\n\u001b[1;32m    860\u001b[0m     handler\u001b[38;5;241m.\u001b[39mreset_changed(changed\u001b[38;5;241m=\u001b[39mreset_changed)\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m canonical\n\u001b[0;32m--> 862\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe connector name \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be found.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(connector_name))\n",
      "\u001b[0;31mConnectionError\u001b[0m: The connector name query can't be found."
     ]
    }
   ],
   "source": [
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "answer = kn.load_canonical('query', query=query)\n",
    "\n",
    "for i in range(answer.num_rows):\n",
    "    s = answer.slice(i,1)\n",
    "    print(f\"Id: {s.column('id')[0]}.as_py()\")\n",
    "    print(f\"Distance: {s.column('distance')[0].as_py()}\")\n",
    "    print(f\"Answer: {print_wrapped(s.column('source')[0].as_py())}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c279bb8-ca82-4e72-93e4-04903f642a35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
