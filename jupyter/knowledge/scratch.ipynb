{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d448bfe-19b5-4fdb-86ce-658e94d2049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves you having to use print as all exposed variables are printed in the cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23954339-a56c-4067-9def-550397dbf8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3338b3bc-a199-441c-a6be-b2c5585e6a70",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f489c0b2-8b59-41fc-9017-f5bbaf2df4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_rag import Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b5d30a2-176e-4a69-8c07-d4957763326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instance of capability\n",
    "kn = Knowledge.from_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69da3e9d-c48d-4fc9-ab52-39ac982cdf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = kn.set_source_uri('hadron/source/Gen AI Best Practices.pdf').load_source_canonical()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "621a8805-bcbb-462d-bad5-5ea9bb749c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = kn.tools.text_to_paragraphs(tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7781770-2083-44f0-b296-b013823cade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kn.tools.str_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44edf25c-3a18-471f-8c82-8966efb4838e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paragraph: Best Practices in Generative AI Responsible use and development  in the modern workplace © Responsible AI Institute 2024   All Rights Reserved\n",
      "\n",
      "\n",
      "paragraph: Do Not Use Without Permission \f",
      "Executive Summary Generative AI, a technology capable of producing realistic content in the form of text, images, sound, and more, presents signiﬁcant opportunities and challenges for businesses today. With generative AI (GenAI) applications ranging from customer service automation to content creation, the recent explosive adoption of LLM technologies like ChatGPT underscores the potential transformative scale of AI impact, both positive and negative. Potential risks and harms from generative AI impact human rights, privacy, security, labor, fairness, sustainability, and more. Without investing effort to comprehensively address these issues across the enterprise, businesses are exposed to the risks of compliance penalties, consumer harm, loss of trust, damages, and more. To position themselves to responsibly capitalize on this potential, organizations must implement governance to pave the way for trustworthy AI deployment, procurement, sale, and use, as applicable. Applying Responsible AI (RAI) frameworks to generative and other forms of AI across the organization can mitigate pressing risks and threats, allowing the technology’s potential to be maximized. The RAI Institute offers the following set of best practices for responsible generative AI use to guide AI practitioners, executive, and other professionals. These guidelines include recommendations related to gathering the right teams and tools, tracking legal requirements, evolving the workforce, and implementing clear objectives and requirements for generative AI. These best practices are grouped into ﬁve categories of Responsible Generative AI: 1. Strategy: Planning, Policies, and Governance 2. Workforce: Training, Education, and Upskilling 3. Capacity: Resourcing and Tools 4. Practice: Development, Procurement, and Use 5. Proactivity: Ongoing Enhancement and Monitoring © Responsible AI Institute 2024\n",
      "\n",
      "\n",
      "paragraph: All Rights Reserved\n",
      "\n",
      "\n",
      "paragraph: Do Not Use Without Permission 2 \f",
      "Background Generative AI is a type of artiﬁcial intelligence (AI) that creates realistic content like images, text,  and videos. It works by using a neural network to learn from a dataset and then generate new  content based on what it “learned.” However, generative AI can cause serious harm if not used  responsibly, such as privacy risks, bias, security threats, lack of transparency, environmental  costs, and more. To optimize returns while mitigating risks, businesses must implement  Responsible AI frameworks grounded in leading standards and best practices. Generative AI Today The launch of ChatGPT in late 2022 precipitated widespread interest in harnessing the  capabilities of generative AI, especially of large language models (LLMs). The use of GenAI to  enable better customer service will be widespread–according to Salesforce’s new generative AI  in IT Survey, 77% of senior IT leaders believe that generative AI will help their organization serve  their customers faster. For example, generative AI can transform customer service by  automating responses to inquiries and providing personalized support, signiﬁcantly reducing  wait times and improving customer satisfaction. It also helps analyze customer feedback in  real-time, enabling businesses to swiftly address concerns and tailor services to meet evolving  needs. Beyond customer service, common uses of GenAI include content creation, where it generates  articles, reports, and creative writing, enhancing productivity across various sectors. In design  and development, GenAI assists in creating software code, architectural plans, and new product  concepts. It also plays a crucial role in data analysis, automating the extraction of insights from  large data sets. Moreover, personalized education and training solutions beneﬁt from GenAI's  ability to adapt learning materials to the user's needs. As organizations adopt GenAI today, considerations and guidelines for responsible use become  increasingly important to address potential biases and ensure privacy and security. © Responsible AI Institute 2024\n",
      "\n",
      "\n",
      "paragraph: All Rights Reserved\n",
      "\n",
      "\n",
      "paragraph: Do Not Use Without Permission © Responsible AI Institute 2024\n",
      "\n",
      "\n",
      "paragraph: All Rights Reserved\n",
      "\n",
      "\n",
      "paragraph: Do Not Use Without Permission 3 \f",
      "Risks of Generative AI Generative AI technology can cause harm if not used responsibly. One risk is that it can be used to mislead people with fake videos and images, such as deepfakes used to scam or misrepresent individuals. Generative AI can also cause harm with biased outputs if it's only trained on information from certain groups, which can lead to unfair and unrepresentative outcomes. For example, when prompted to describe or depict a “professional person in the workplace,” an AI system that was trained on biased data might omit photos of women, particularly women of color. Furthermore, AI systems can generate factually inaccurate outputs, even making up or “hallucinating” research reports, laws, or historical events in their outputs. This hallucination occurs when an AI system learns from data and produces its own new, plausible-seeming but fabricated information. This can occur due to data quality and mitigation issues such as biased or limited training data and model overﬁtting in response to the data. There are also other risks associated with generative AI, such as security problems. For example, generative AI can rely on large-scale datasets that hold private information about individuals that can be elicited through prompts. This also poses intellectual property concerns in terms of both the inputs and outputs to the AI. For example, a recent study of over 10,000 employees found that 15 percent of employees input company data into ChatGPT, putting their company at risk of a security breach. Security breaches can happen due to a generative AI system’s vulnerability to threats such as model theft, data poisoning, and adversarial attacks. Another risk is a lack of transparency in the AI’s decision-making processes, which can confuse both users and developers on a system’s outputs and blur the lines of legal liability. According to a Salesforce survey, many IT leaders share a variety of concerns related to implementing GenAI in their organizations. 79% of leaders were concerned about potential security risks, 59% believed generative AI outputs are inaccurate, and 63% believed that there is bias in generative AI outputs, including misinformation and hate speech. 71% of leaders also believed that generative AI would increase their carbon footprint through increased IT energy use. LLMs also pose new risks due to their training style and propensity to be procured for third-party use and or integrated into downstream applications. LLMs are notoriously obscure across explainability and interpretability dimensions, leading to unpredictable behaviors and vulnerabilities. There are also privacy and copyright concerns related to the use of users’ and data subjects’ data for training, which may lead to legal challenges and penalties in near future. © Responsible AI Institute 2024\n",
      "\n",
      "\n",
      "paragraph: All Rights Reserved\n",
      "\n",
      "\n",
      "paragraph: Do Not Use Without Permission 4 \f",
      "Why a Responsible Approach Generative AI In response to the myriad risks that generative AI poses to both the ﬁnancial and reputational  standing of organizations, as well as to the safety and rights of the public, organizations are  urged to incorporate Responsible AI principles into their operations and product development.  Responsible AI represents a comprehensive, stakeholder-driven methodology for the design,  deployment, and implementation of technology. It emphasizes adherence to regulations, laws,  and organizational values as central to AI creation and decision-making processes, applicable at  both organizational and product levels. AI technologies, by their nature, carry inherent risks. The decisions shaping the design,  development, deployment, evaluation, and utilization of AI systems often reﬂect systemic biases  and human cognitive limitations. These risks extend beyond individual developers, affecting  entire organizations and potentially leading to widespread societal impacts. Traditional  corporate governance structures are ill-equipped to keep pace with AI's rapid development, and  existing risk management frameworks fail to address the unique challenges AI systems  introduce. AI technologies present both new and ampliﬁed risks compared to traditional software. For  instance, the data fueling AI systems might not accurately reﬂect the intended context or use,  potentially lacking a reliable ground truth and introducing harmful biases. The reliance on  extensive, complex data for training, alongside the potential for signiﬁcant changes during this  process, underscores the uniqueness of AI-speciﬁc risks. These include the detachment of  training datasets from their intended context, the enormous scale and complexity of AI systems,  and the challenges associated with managing pre-trained models. Furthermore, AI systems face unique challenges such as increased statistical uncertainty, bias  management issues, privacy risks due to enhanced data aggregation capabilities, and the  necessity for more frequent maintenance. The opacity of AI systems and concerns over  reproducibility, coupled with underdeveloped standards for software testing and documentation,  highlight the need for AI-speciﬁc risk management and strategic planning. These strategies  must clearly articulate objectives and delineate human roles and responsibilities in overseeing  AI systems, ensuring that organizations can navigate the complexities of generative AI  responsibly and effectively. Furthermore, demand for responsible trustworthy AI is only growing. A study by Edelman  showed that 81% of consumers prefer purchasing from companies that prioritize data privacy  and security. Furthermore, research by PwC found that 60% of consumers are more likely to  trust companies that are transparent about their AI use. According to a report by the Deloitte AI  Institute and US Chamber of Commerce, a trustworthy AI approach “can mitigate risks that © Responsible AI Institute 2024\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in p['paragraph'][:10]:\n",
    "    print(f'paragraph: {item}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7ce8b9-e40b-46bb-a5d1-fcca9e377a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_elements(input_list):\n",
    "    for i in range(len(input_list) - 1):\n",
    "        current_element = input_list[i]\n",
    "        next_element = input_list[i + 1]\n",
    "        \n",
    "        if current_element < next_element:\n",
    "            print(f\"{current_element} is less than {next_element}\")\n",
    "        elif current_element == next_element:\n",
    "            print(f\"{current_element} is equal to {next_element}\")\n",
    "        else:\n",
    "            print(f\"{current_element} is greater than {next_element}\")\n",
    "\n",
    "# Example usage\n",
    "example_list = [3, 5, 2, 8, 6]\n",
    "compare_elements(example_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a1995a3-3056-475f-87ae-1ff6fdde0fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nouns - [('use', 1), ('development', 1), ('workplace', 1), ('Rights', 1)]\n",
      "nouns - [('risks', 3), ('technology', 2), ('content', 2), ('businesses', 2), ('impact', 2)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('customer', 5), ('content', 3), ('service', 3), ('privacy', 2), ('risks', 2)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('Permission', 1), ('©', 1)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('data', 9), ('outputs', 7), ('system', 4), ('risk', 3), ('information', 3)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('risks', 7), ('systems', 7), ('organizations', 4), ('data', 4), ('development', 3)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('Permission', 1), ('©', 1)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('systems', 3), ('applications', 2), ('organizations', 2), ('strategies', 2), ('use', 2)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('practices', 6), ('organizations', 3), ('development', 3), ('use', 2), ('guide', 1)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('team', 7), ('requirements', 4), ('deployment', 3), ('standardization', 3), ('development', 3)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('model', 5), ('organization', 4), ('models', 4), ('resources', 3), ('contracts', 3)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('data', 16), ('model', 8), ('quality', 5), ('considerations', 4), ('training', 4)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('governance', 6), ('development', 6), ('policies', 5), ('deployment', 4), ('policy', 4)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('risk', 8), ('management', 5), ('systems', 4), ('risks', 4), ('approach', 3)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('training', 8), ('roles', 4), ('compliance', 4), ('resources', 3), ('speciﬁc', 3)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('model', 6), ('function', 5), ('training', 3), ('information', 3), ('team', 3)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('cybersecurity', 5), ('organization', 4), ('security', 3), ('models', 2), ('teams', 2)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('resources', 5), ('tools', 5), ('system', 5), ('management', 4), ('processes', 4)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('development', 14), ('model', 7), ('system', 5), ('risk', 4), ('data', 4)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('model', 10), ('development', 6), ('models', 5), ('performance', 4), ('outputs', 3)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('procurement', 4), ('use', 4), ('model', 3), ('suppliers', 3), ('organization', 3)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('information', 10), ('employees', 8), ('systems', 5), ('impacts', 2), ('guidance', 2)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('systems', 4), ('monitoring', 3), ('audits', 3), ('risks', 2), ('organization', 2)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('beneﬁts', 3), ('risks', 3), ('business', 3), ('technology', 2), ('privacy', 2)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('technologies', 3), ('content', 3), ('data', 2), ('deﬁnitions', 2), ('ﬁeld', 2)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('bias', 8), ('systems', 7), ('data', 6), ('system', 5), ('management', 4)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('data', 13), ('process', 7), ('system', 5), ('governance', 4), ('Data', 3)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('system', 5), ('design', 5), ('risk', 5), ('governance', 4), ('processes', 4)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('characteristics', 7), ('systems', 4), ('system', 4), ('Source', 3), ('accountability', 2)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('system', 6), ('systems', 6), ('conditions', 3), ('use', 3), ('time', 2)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('system', 6), ('biases', 5), ('decision', 4), ('bias', 4), ('Privacy', 3)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('system', 11), ('concepts', 7), ('Covers', 6), ('product', 6), ('team', 4)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('organizations', 2), ('member', 2), ('members', 2), ('journeys', 1), ('support', 1)]\n",
      "nouns - [('Rights', 1)]\n",
      "nouns - [('Permission', 1)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "for text in p['paragraph']:\n",
    "    doc = nlp(text.as_py())\n",
    "    \n",
    "    # noun tokens that arent stop words or punctuations\n",
    "    nouns = [token.text\n",
    "             for token in doc\n",
    "             if (not token.is_stop and\n",
    "                 not token.is_punct and\n",
    "                 token.pos_ == \"NOUN\")]\n",
    "    \n",
    "    # five most common tokens\n",
    "    word_freq = Counter(words)\n",
    "    common_words = word_freq.most_common(5)\n",
    "    \n",
    "    # five most common noun tokens\n",
    "    noun_freq = Counter(nouns)\n",
    "    common_nouns = noun_freq.most_common(5)\n",
    "    print(f\"nouns - {common_nouns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aac0434-a9ca-4875-8ef6-cc1d525a5ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
