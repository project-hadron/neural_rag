{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d448bfe-19b5-4fdb-86ce-658e94d2049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves you having to use print as all exposed variables are printed in the cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23954339-a56c-4067-9def-550397dbf8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3338b3bc-a199-441c-a6be-b2c5585e6a70",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f489c0b2-8b59-41fc-9017-f5bbaf2df4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_rag import Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b5d30a2-176e-4a69-8c07-d4957763326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instance of capability\n",
    "kn = Knowledge.from_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69da3e9d-c48d-4fc9-ab52-39ac982cdf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = kn.set_source_uri('hadron/source/Gen AI Best Practices.pdf').load_source_canonical()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "621a8805-bcbb-462d-bad5-5ea9bb749c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = kn.tools.text_to_paragraphs(tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7781770-2083-44f0-b296-b013823cade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = kn.tools.str_remove_text(p, pattern='^All Rights Reserved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44edf25c-3a18-471f-8c82-8966efb4838e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b9632 th {\n",
       "  font-size: 120%;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_b9632 .row_heading {\n",
       "  display: none;;\n",
       "}\n",
       "#T_b9632  .blank {\n",
       "  display: none;;\n",
       "}\n",
       "#T_b9632_row0_col0, #T_b9632_row1_col0, #T_b9632_row2_col0, #T_b9632_row3_col0, #T_b9632_row4_col0, #T_b9632_row5_col0, #T_b9632_row6_col0, #T_b9632_row7_col0, #T_b9632_row8_col0, #T_b9632_row9_col0 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b9632\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b9632_level0_col0\" class=\"col_heading level0 col0\" >text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b9632_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_b9632_row0_col0\" class=\"data row0 col0\" >Best Practices in Generative AI Responsible use and development  in the modern workplace © Responsible AI Institute 2024   All Rights Reserved.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9632_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_b9632_row1_col0\" class=\"data row1 col0\" >Do Not Use Without Permission \f",
       "Executive Summary Generative AI, a technology capable of producing realistic content in the form of text, images, sound, and more, presents signiﬁcant opportunities and challenges for businesses today. With generative AI (GenAI) applications ranging from customer service automation to content creation, the recent explosive adoption of LLM technologies like ChatGPT underscores the potential transformative scale of AI impact, both positive and negative. Potential risks and harms from generative AI impact human rights, privacy, security, labor, fairness, sustainability, and more. Without investing effort to comprehensively address these issues across the enterprise, businesses are exposed to the risks of compliance penalties, consumer harm, loss of trust, damages, and more. To position themselves to responsibly capitalize on this potential, organizations must implement governance to pave the way for trustworthy AI deployment, procurement, sale, and use, as applicable. Applying Responsible AI (RAI) frameworks to generative and other forms of AI across the organization can mitigate pressing risks and threats, allowing the technology’s potential to be maximized. The RAI Institute offers the following set of best practices for responsible generative AI use to guide AI practitioners, executive, and other professionals. These guidelines include recommendations related to gathering the right teams and tools, tracking legal requirements, evolving the workforce, and implementing clear objectives and requirements for generative AI. These best practices are grouped into ﬁve categories of Responsible Generative AI: 1. Strategy: Planning, Policies, and Governance 2. Workforce: Training, Education, and Upskilling 3. Capacity: Resourcing and Tools 4. Practice: Development, Procurement, and Use 5. Proactivity: Ongoing Enhancement and Monitoring © Responsible AI Institute 2024.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9632_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_b9632_row2_col0\" class=\"data row2 col0\" >Do Not Use Without Permission 2 \f",
       "Background Generative AI is a type of artiﬁcial intelligence (AI) that creates realistic content like images, text,  and videos. It works by using a neural network to learn from a dataset and then generate new  content based on what it “learned.” However, generative AI can cause serious harm if not used  responsibly, such as privacy risks, bias, security threats, lack of transparency, environmental  costs, and more. To optimize returns while mitigating risks, businesses must implement  Responsible AI frameworks grounded in leading standards and best practices. Generative AI Today The launch of ChatGPT in late 2022 precipitated widespread interest in harnessing the  capabilities of generative AI, especially of large language models (LLMs). The use of GenAI to  enable better customer service will be widespread–according to Salesforce’s new generative AI  in IT Survey, 77% of senior IT leaders believe that generative AI will help their organization serve  their customers faster. For example, generative AI can transform customer service by  automating responses to inquiries and providing personalized support, signiﬁcantly reducing  wait times and improving customer satisfaction. It also helps analyze customer feedback in  real-time, enabling businesses to swiftly address concerns and tailor services to meet evolving  needs. Beyond customer service, common uses of GenAI include content creation, where it generates  articles, reports, and creative writing, enhancing productivity across various sectors. In design  and development, GenAI assists in creating software code, architectural plans, and new product  concepts. It also plays a crucial role in data analysis, automating the extraction of insights from  large data sets. Moreover, personalized education and training solutions beneﬁt from GenAI's  ability to adapt learning materials to the user's needs. As organizations adopt GenAI today, considerations and guidelines for responsible use become  increasingly important to address potential biases and ensure privacy and security. © Responsible AI Institute 2024.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9632_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_b9632_row3_col0\" class=\"data row3 col0\" >Do Not Use Without Permission © Responsible AI Institute 2024.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9632_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_b9632_row4_col0\" class=\"data row4 col0\" >Do Not Use Without Permission 3 \f",
       "Risks of Generative AI Generative AI technology can cause harm if not used responsibly. One risk is that it can be used to mislead people with fake videos and images, such as deepfakes used to scam or misrepresent individuals. Generative AI can also cause harm with biased outputs if it's only trained on information from certain groups, which can lead to unfair and unrepresentative outcomes. For example, when prompted to describe or depict a “professional person in the workplace,” an AI system that was trained on biased data might omit photos of women, particularly women of color. Furthermore, AI systems can generate factually inaccurate outputs, even making up or “hallucinating” research reports, laws, or historical events in their outputs. This hallucination occurs when an AI system learns from data and produces its own new, plausible-seeming but fabricated information. This can occur due to data quality and mitigation issues such as biased or limited training data and model overﬁtting in response to the data. There are also other risks associated with generative AI, such as security problems. For example, generative AI can rely on large-scale datasets that hold private information about individuals that can be elicited through prompts. This also poses intellectual property concerns in terms of both the inputs and outputs to the AI. For example, a recent study of over 10,000 employees found that 15 percent of employees input company data into ChatGPT, putting their company at risk of a security breach. Security breaches can happen due to a generative AI system’s vulnerability to threats such as model theft, data poisoning, and adversarial attacks. Another risk is a lack of transparency in the AI’s decision-making processes, which can confuse both users and developers on a system’s outputs and blur the lines of legal liability. According to a Salesforce survey, many IT leaders share a variety of concerns related to implementing GenAI in their organizations. 79% of leaders were concerned about potential security risks, 59% believed generative AI outputs are inaccurate, and 63% believed that there is bias in generative AI outputs, including misinformation and hate speech. 71% of leaders also believed that generative AI would increase their carbon footprint through increased IT energy use. LLMs also pose new risks due to their training style and propensity to be procured for third-party use and or integrated into downstream applications. LLMs are notoriously obscure across explainability and interpretability dimensions, leading to unpredictable behaviors and vulnerabilities. There are also privacy and copyright concerns related to the use of users’ and data subjects’ data for training, which may lead to legal challenges and penalties in near future. © Responsible AI Institute 2024.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9632_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_b9632_row5_col0\" class=\"data row5 col0\" >Do Not Use Without Permission 4 \f",
       "Why a Responsible Approach Generative AI In response to the myriad risks that generative AI poses to both the ﬁnancial and reputational  standing of organizations, as well as to the safety and rights of the public, organizations are  urged to incorporate Responsible AI principles into their operations and product development.  Responsible AI represents a comprehensive, stakeholder-driven methodology for the design,  deployment, and implementation of technology. It emphasizes adherence to regulations, laws,  and organizational values as central to AI creation and decision-making processes, applicable at  both organizational and product levels. AI technologies, by their nature, carry inherent risks. The decisions shaping the design,  development, deployment, evaluation, and utilization of AI systems often reﬂect systemic biases  and human cognitive limitations. These risks extend beyond individual developers, affecting  entire organizations and potentially leading to widespread societal impacts. Traditional  corporate governance structures are ill-equipped to keep pace with AI's rapid development, and  existing risk management frameworks fail to address the unique challenges AI systems  introduce. AI technologies present both new and ampliﬁed risks compared to traditional software. For  instance, the data fueling AI systems might not accurately reﬂect the intended context or use,  potentially lacking a reliable ground truth and introducing harmful biases. The reliance on  extensive, complex data for training, alongside the potential for signiﬁcant changes during this  process, underscores the uniqueness of AI-speciﬁc risks. These include the detachment of  training datasets from their intended context, the enormous scale and complexity of AI systems,  and the challenges associated with managing pre-trained models. Furthermore, AI systems face unique challenges such as increased statistical uncertainty, bias  management issues, privacy risks due to enhanced data aggregation capabilities, and the  necessity for more frequent maintenance. The opacity of AI systems and concerns over  reproducibility, coupled with underdeveloped standards for software testing and documentation,  highlight the need for AI-speciﬁc risk management and strategic planning. These strategies  must clearly articulate objectives and delineate human roles and responsibilities in overseeing  AI systems, ensuring that organizations can navigate the complexities of generative AI  responsibly and effectively. Furthermore, demand for responsible trustworthy AI is only growing. A study by Edelman  showed that 81% of consumers prefer purchasing from companies that prioritize data privacy  and security. Furthermore, research by PwC found that 60% of consumers are more likely to  trust companies that are transparent about their AI use. According to a report by the Deloitte AI  Institute and US Chamber of Commerce, a trustworthy AI approach “can mitigate risks that © Responsible AI Institute 2024.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9632_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_b9632_row6_col0\" class=\"data row6 col0\" >Do Not Use Without Permission © Responsible AI Institute 2024.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9632_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_b9632_row7_col0\" class=\"data row7 col0\" >Do Not Use Without Permission 5 \f",
       "might otherwise reduce conﬁdence in AI systems and stiﬂe innovation in this critical sector  while focusing investment on beneﬁcial applications of AI that can lead to economic growth and  improved health, safety, and well-being.” Investment in developing RAI maturity saves  organizations by improving trust, market advantage, and more. Becoming a Leader in Responsible Generative AI Leaders in the generative AI ﬁeld have implemented a variety of strategies to ensure the  responsible use and development of AI technologies within their organizations. They have  disseminated enterprise-wide guidance on best practices, regulations, and intellectual property  considerations related to generative AI. Additionally, they've fostered a culture of continuous  learning and knowledge sharing through initiatives such as lunch-and-learns, dedicated Slack  channels, intranet pages, and repositories. These platforms allow employees to discuss and  deepen their understanding of generative AI. To further commit to trustworthy AI use, these leaders have established internal Responsible AI  review boards, incorporating external subject matter experts to align the company's AI  applications with its principles and objectives. They've developed comprehensive procedures for  tracking and researching leading AI technologies, alongside relevant regulations and mitigation  strategies. This includes creating AI-speciﬁc procurement and vendor evaluation processes to  responsibly acquire third-party AI systems. Employees are encouraged to engage in professional  development opportunities related to AI, including training, pilot projects, and contributions to  standards and open-source technology research. Moreover, pursuing Responsible AI  certiﬁcation for their systems, in line with national accreditation standards and leading  practices, underscores their dedication to responsible AI deployment. © Responsible AI Institute 2024.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9632_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_b9632_row8_col0\" class=\"data row8 col0\" >Do Not Use Without Permission 6 \f",
       "Best Practices This RAI Institute guide serves as a resource for organizations aiming to foster responsible use of generative AI. It outlines a non-exhaustive list of best practices in GenAI use, procurement, and development. These best practices should be implemented in parallel where appropriate by diverse, inclusive, and cross-functional teams. These best practices are grouped into ﬁve categories of Responsible Generative AI: 1. Strategy: This encompasses Planning, Policies, and Governance, ensuring that the organization's GenAI initiatives are well-aligned with its overall goals and compliant with relevant regulations. 2. Workforce: Focuses on Training, Education, and Upskilling, equipping employees with the necessary knowledge and skills to effectively engage with GenAI technologies. 3. Capacity: Relates to Resourcing and Tools, addressing the need for adequate resources and tools to support GenAI development and deployment. 4. Practice: Covers Development, Procurement, and Use, guiding organizations on responsible GenAI creation, acquisition, and application. 5. Proactivity: Emphasizes Ongoing Enhancement and Monitoring, advocating for continuous improvement and vigilance in GenAI practices. By adhering to these best practices, organizations can more effectively navigate the complexities of GenAI responsibly, ensuring that their efforts are both fruitful and aligned with broader responsible AI and regulatory standards. Given the rapid evolution of GenAI technology and the continuous development of best practices, this document is regularly updated to remain current and relevant. © Responsible AI Institute 2024.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9632_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_b9632_row9_col0\" class=\"data row9 col0\" >Do Not Use Without Permission 7 \f",
       "Responsible Generative AI Strategy: Planning, Policies, and Governance The strategic deployment of Generative AI necessitates an integrated tiered approach.  Developing a responsible GenAI strategy will involve assembling a cross-functional team to  centralize AI expertise and capabilities for enhanced collaboration, governance, and  standardization across the organization. In strategic planning, organizations should involve  external experts and stakeholders to ensure comprehensive AI development and deployment,  alongside clarifying legal requirements to navigate the evolving GenAI landscape and deﬁning  clear objectives, use cases, and requirements for GenAI solutions to align with business goals  and responsible AI standards. Assemble a cross-functional team to further AI activities Centralizing existing AI experience and capabilities, including GenAI in a cross-functional team,  allows for eﬃcient knowledge sharing, collaboration, and standardization across different  departments and business units. This team, potentially crystallized into a Responsible GenAI  Committee, Council, or other formal body, should align ongoing efforts and be accountable for  AI governance, standardization of development processes, resource pooling, and internal  expertise for GenAI projects. In its scope, this team should be tasked with steering upskilling  initiatives, developing a long-term AI roadmap aligned with company objectives, and promoting  innovation through challenges and competitions. ● Consider the following roles of functions when assembling this team: AI Product Owner and/or Business Lead, Machine Learning / AI analyst Engineer or Developer, Responsible AI, Data Governance, Legal, Risk, Security, Privacy, Procurement, Internal Audit. ● Additionally, engage external actors, including subject matter experts, users, and at-risk groups, to ensure a comprehensive approach to AI development and deployment within the organization. Clarify and map legal requirements Legal and regulatory requirements involving AI and generative AI must be well understood,  managed, and documented. Since GenAI is a rapidly evolving ﬁeld, input from a specialized legal  team is needed to not only maintain already well-documented IP and user privacy controls, but  to also monitor the legal landscape with respect to upcoming GenAI rulings, laws, and  regulations. © Responsible AI Institute 2024.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fa632187280>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kn.table_report(p, headers='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7ce8b9-e40b-46bb-a5d1-fcca9e377a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_elements(input_list):\n",
    "    for i in range(len(input_list) - 1):\n",
    "        current_element = input_list[i]\n",
    "        next_element = input_list[i + 1]\n",
    "        \n",
    "        if current_element < next_element:\n",
    "            print(f\"{current_element} is less than {next_element}\")\n",
    "        elif current_element == next_element:\n",
    "            print(f\"{current_element} is equal to {next_element}\")\n",
    "        else:\n",
    "            print(f\"{current_element} is greater than {next_element}\")\n",
    "\n",
    "# Example usage\n",
    "example_list = [3, 5, 2, 8, 6]\n",
    "compare_elements(example_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1995a3-3056-475f-87ae-1ff6fdde0fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "for text in p['paragraph']:\n",
    "    doc = nlp(text.as_py())\n",
    "    \n",
    "    # noun tokens that arent stop words or punctuations\n",
    "    nouns = [token.text\n",
    "             for token in doc\n",
    "             if (not token.is_stop and\n",
    "                 not token.is_punct and\n",
    "                 token.pos_ == \"NOUN\")]\n",
    "    \n",
    "    # five most common tokens\n",
    "    word_freq = Counter(words)\n",
    "    common_words = word_freq.most_common(5)\n",
    "    \n",
    "    # five most common noun tokens\n",
    "    noun_freq = Counter(nouns)\n",
    "    common_nouns = noun_freq.most_common(5)\n",
    "    print(f\"nouns - {common_nouns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aac0434-a9ca-4875-8ef6-cc1d525a5ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
